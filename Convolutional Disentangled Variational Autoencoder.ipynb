{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch 4.1+\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional β-Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "\n",
    "ZDIMS = 20\n",
    "BETA = 5\n",
    "LR = 1e-3\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "SEED = 4\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if CUDA else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train = True,\n",
    "                   download = True,\n",
    "                   transform = transforms.ToTensor()),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# build test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train = False,\n",
    "                   transform = transforms.ToTensor()),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Because I plan on using this for dimension reduction, I based the architecture loosely on Convolutional Variational Autoencoder featured in the influential World Models research paper. It has been modified with a β coefficient for disentangling the latent features/vectors.\n",
    " - https://worldmodels.github.io/\n",
    " - https://github.com/hardmaru/WorldModelsExperiments\n",
    " - https://github.com/hardmaru/WorldModelsExperiments/blob/master/carracing/vae/vae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC = 1    # channels\n",
    "NEF = 64  # init encoding filters\n",
    "NDF = 64  # init decoding filters\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, zdims):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zdims = zdims\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            # input is (NC) x 28 x 28 (MNIST)\n",
    "            nn.Conv2d(in_channels = NC, out_channels = NEF, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 2\n",
    "            nn.Conv2d(in_channels = NEF, out_channels = NEF * 2, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NEF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 3\n",
    "            nn.Conv2d(in_channels = NEF * 2, out_channels = NEF * 4, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NEF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 4\n",
    "            nn.Conv2d(in_channels = NEF * 4, out_channels = 1024, kernel_size = 4, stride = 2, padding = 1),\n",
    "            #nn.BatchNorm2d(1024), # OPTIONAL\n",
    "            nn.ReLU(inplace = True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            # input is Z (post-fc)\n",
    "            nn.ConvTranspose2d(in_channels = 1024, out_channels = NDF * 8, kernel_size = 4, stride = 1, padding = 0),\n",
    "            nn.BatchNorm2d(NDF * 8),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 2\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 8, out_channels = NDF * 4, kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NDF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 3\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 4, out_channels = NDF * 2, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NDF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 4\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 2, out_channels = NC, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        \n",
    "        # conv fc\n",
    "        self.fc11 = nn.Linear(1024, self.zdims) # mu\n",
    "        self.fc12 = nn.Linear(1024, self.zdims) # logvar\n",
    "        \n",
    "        # deconv fc\n",
    "        self.fc2  = nn.Linear(self.zdims, 1024)\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        conv = self.encoder(x)\n",
    "        conv = conv.view(-1, 1024)\n",
    "        mu = self.fc11(conv)\n",
    "        logvar = self.fc12(conv)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        deconv_input = F.relu(self.fc2(z))\n",
    "        deconv_input = deconv_input.view(-1, 1024, 1, 1) # world models: [-1, 1, 1, 1024]\n",
    "        recon_x = self.decoder(deconv_input)\n",
    "        return recon_x\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(logvar * 0.5)\n",
    "        eps = torch.rand_like(std)\n",
    "        z = eps.mul(std).add(mu)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(zdims = ZDIMS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta = 1):\n",
    "    '''Use a beta value of 1 for a vanilla VAE'''\n",
    "    \n",
    "    # loss\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction = 'sum')\n",
    "    \n",
    "    # KL Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + (beta * KLD)\n",
    "\n",
    "def train(epoch, beta = 1):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, Z = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)\n",
    "            ))\n",
    "    \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch,\n",
    "        train_loss / len(train_loader.dataset)\n",
    "    ))\n",
    "    \n",
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar, Z = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow = n)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, beta = BETA) # note: beta not currently considered in validation error\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from latent space\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(64, ZDIMS).to(device)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Results\n",
    "\n",
    "*NOTE: because the input data (MNIST vs World Models) is very different, most of the layer hyperparams had to be adjusted. Otherwise, the biggest difference is that Batch Normalization, which showed a lower training and validation error, is used.\n",
    "\n",
    " - **Modified World Models**\n",
    "     - val loss = 78.1520 (min: 77.5991)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = False\n",
    " - **Modified World Models + BatchNorm (not on last encoding layer)**\n",
    "     - val loss = 76.4446 (min: 76.0836)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = True\n",
    " - **Modified World Models + BatchNorm (including last encoding layer)**\n",
    "     - val loss = 76.3589 (min: 76.1684)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = True\n",
    " - **Modified World Models + BatchNorm (not on last encoding layer) w/ beta**\n",
    "     - val loss = 85.2611 (min: 85.2245)\n",
    "     - zdims = 20\n",
    "     - beta = 5\n",
    "     - batch size = 64\n",
    "     - batchnorm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Although the results are good, it is unclear from the validation error alone whether or not the latent vectors are being disentangled in a way that features can be extracted. Next I will use the latent vectors at various beta levels to test the classification effectiveness/accuracy. Additionally, while the VAE generalizes well, it is not effective in situations when the reconstructed image's quality (clarity / bluriness) cannot be sacrificed -- for this reason, I will also test the performance of the MMD-VAE (InfoVAE).\n",
    "\n",
    "https://arxiv.org/pdf/1706.02262.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
